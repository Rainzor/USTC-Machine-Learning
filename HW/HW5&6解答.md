# HW5

#### 5.1 

  **试述将线性函数 $f(x) = w^\top x$ 用作神经元激活函数的缺陷** 

#### Sol.

（言之有理即可）
解：当激活函数为线性函数 $f(x) = w^\top x$时，每一层输出都是上层输入的线性函数，
无论神经网络有多少层，输出都是输入的线性组合，此时设置多层对逼近函数没有额外的贡献，神经网络实际仍为原始的感知机，无法处
理非线性问题。使用非线性激活函数增加了神经网络模型的非线性因素，使得神经网络可以
任意逼近任何非线性函数，可以应用到非线性模型中。

#### 2.
​	
**讨论 $\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}$ 和 $\log\sum_{j=1}\exp(x_j)$ 的数值溢出问题**

#### Sol.

实数在计算机中以二进制表示，计算时非精确值。当数值过小会取0（下溢出）或数值过
大导致上溢出。在Softmax情境下的解决方案为：

令 $M=\max(x_i), i\in\{1,\cdots, C\}$ ，此时计算上述两值为，

$$
\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}=\frac{\exp(x_i-M)}{\sum_{j=1}\exp(x_j-M)}
$$

$$
\log\sum_{j=1}\exp(x_j) = M +\log\sum_{j=1}\exp(x_j-M)
$$

此时 $\exp(x_i-M)\leq \exp(M-M)=1$ , 不会发生上溢出；

$\frac{1}{\sum_{j=1}\exp(x_j-M)}\geq\frac{1}{\sum_{j=1}\exp(M-M)}=\frac{1}{C}$, 不会发生下溢出。

#### 3. 
​	
**计算 $\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}$ 和 $\log\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}$ 关于向量 $\boldsymbol{x} = [x_1,\cdots,x_C]$ 的梯度**

#### Sol.

记 $f(x_i) = \frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}, g(x_i) = \log f(x_i)$,则有

$$
\frac{\partial f(x_i)}{x_i} = \frac{\exp(x_i)\sum_{j=1}\exp(x_j)-\exp(x_i)^2}{(\sum_{j=1}\exp(x_j))^2}=\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}-\frac{\exp(x_i)^2}{(\sum_{j=1}\exp(x_j))^2} = f(x_i)(1-f(x_i))
$$

$$
\frac{\partial f(x_i)}{x_j} = -f(x_i)f(x_j), \forall j\neq i 
$$

$$
\implies \frac{\partial f(x_i)}{\partial \boldsymbol{x}}=f(x_i)[-f(x_1),\cdots,-f(x_{i-1}),1-f(x_i),-f(x_{i+1}),\cdots,-f(x_C)]
$$

$$
\frac{\partial g(x_i)}{\partial \boldsymbol{x}} = [-f(x_1),\cdots,-f(x_{i-1}),1-f(x_i),-f(x_{i+1}),\cdots,-f(x_C)]
$$

#### 4.

**考虑如下简单网络,假设激活函数为ReLU ,用平方损失 $\frac{1}{2}(y − \hat{y})^2$计算误差，请用BP算法更新一次所有参数 (学习率为1)，给出更新后的参数值(给出详细计算过程)，并计算给定输入值$x = (0.2, 0.3)$时初始时和更新后的输出值,检查参数更新是否降低了平方损失值.**

#### Sol:

$$
ReLU(x) = \begin{cases}x, x\gt 0 \\ 0, x \leq 0\end{cases}
$$

