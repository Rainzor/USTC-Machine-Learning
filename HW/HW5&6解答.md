# HW5

#### 5.1 

  **试述将线性函数 $f(x) = w^\top x$ 用作神经元激活函数的缺陷** 

#### Sol.

（言之有理即可）
解：当激活函数为线性函数 $f(x) = w^\top x$时，每一层输出都是上层输入的线性函数，
无论神经网络有多少层，输出都是输入的线性组合，此时设置多层对逼近函数没有额外的贡献，神经网络实际仍为原始的感知机，无法处
理非线性问题。使用非线性激活函数增加了神经网络模型的非线性因素，使得神经网络可以
任意逼近任何非线性函数，可以应用到非线性模型中。

#### 2.
​	**讨论 $\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}$ 和 $\log\sum_{j=1}\exp(x_j)$ 的数值溢出问题**

#### Sol.

实数在计算机中以二进制表示，计算时非精确值。当数值过小会取0（下溢出）或数值过
大导致上溢出。在Softmax情境下的解决方案为：

令 $M=\max(x_i), i\in\{1,\cdots, C\}$ ，此时计算上述两值为，

$$
\frac{\exp(x_i)}{\sum_{j=1}\exp(x_j)}=\frac{\exp(x_i-M)}{\sum_{j=1}\exp(x_j-M)}
$$

$$
\log\sum_{j=1}\exp(x_j) = M +\log\sum_{j=1}\exp(x_j-M)
$$

此时 $\exp(x_i-M)\leq \exp(M-M)=1$ , 不会发生上溢出；

$\frac{1}{\sum_{j=1}\exp(x_j-M)}\geq\frac{1}{\sum_{j=1}\exp(M-M)}=\frac{1}{C}$, 不会发生下溢出。

#### 3. 
​	**在 LDA 多分类情形下, 试计算类间散度矩阵 $S_b$ 的秩, 并证明**

#### Sol.

$$
\boldsymbol{S}_b=\sum_c m_c\left(\boldsymbol{\mu}_c-\boldsymbol{\mu}\right)\left(\boldsymbol{\mu}_c-\boldsymbol{\mu}\right)^{\top}=\boldsymbol{A} \boldsymbol{M} \boldsymbol{A}^{\top}
$$

其中

$$
\boldsymbol{A}=\left(\begin{array}{llll}
\boldsymbol{\mu}_1-\boldsymbol{\mu} & \boldsymbol{\mu}_2-\boldsymbol{\mu} & \cdots & \boldsymbol{\mu}_N-\boldsymbol{\mu}
\end{array}\right), \quad \boldsymbol{M}=\text{diag}\left(m_1, m_2, \cdots, m_N\right) .
$$

接着, 可以得到

$$
\text{rank} \boldsymbol{S}_b=\text{rank}\left(\boldsymbol{A} \boldsymbol{M A}^{\top}\right)=\text{rank}\left(\left(\boldsymbol{A} \boldsymbol{M}^{\frac{1}{2}}\right)\left(\boldsymbol{A} \boldsymbol{M}^{\frac{1}{2}}\right)^{\top}\right)=\text{rank}\left(\boldsymbol{A} \boldsymbol{M}^{\frac{1}{2}}\right)=\text{rank} \boldsymbol{A}
$$

因为 $\sum_c m_i\left(\boldsymbol{\mu}_i-\boldsymbol{\mu}\right)=\mathbf{0}$, 所以 $\text{rank} \boldsymbol{S}_b=\text{rank} \boldsymbol{A} \leq \min(N-1,D)$. 

其中 D为数据点维度

#### 4 

​	**给出公式 (3.45) 的推导公式**

#### Sol.

书本此处 $W$是矩阵，不是向量，显然 $\lambda$表示有误，

$\max \text{tr}\left(W^{\top} S_b W\right) s.t. \text{tr}\left(W^{\top} S_W W\right)=1$
同样也等价子如下目标:

$$
\max \text{tr}\left(W^{\top} S_bW\right) \text { s.t. } W^{\top} S_W W=1
$$

利用 Lagrange乘子法: $L(W, \Lambda)=-\text{tr}\left[W^{\top} S_bW\right]+\text{tr}\left[\Lambda^{\top}\left(W^{\top} S_W W-I\right)\right]$

$$
\begin{aligned}
& \frac{\partial L(W, \Lambda)}{\partial W}=-2 S_b W+2 S_W W \Lambda=0 \\
& \therefore S_b W=S_W W \Lambda \text {. } \\
&
\end{aligned}
$$




#### 5. 
​	**证明 $X\left(X^{\top} X\right)^{-1} X^{\top}$ 是投影矩阵, 并对线性回归模型从投影角度解释。**



证明. 令 $P=X\left(X^{\top} X\right)^{-1} X^{\top}$, 那么

$$
P^{\top}=\left(X\left(X^{\top} X\right)^{-1} X^{\top}\right)^{\top}=X\left(X\left(X^{\top} X\right)^{-1}\right)^{\top}=X\left(X^{\top} X\right)^{-1} X^{\top}=P
$$

因此 $P$ 是一个对称矩阵, 又因为

$$
P^2=X\left(X^{\top} X\right)^{-1} X^{\top} X\left(X^{\top} X\right)^{-1} X^{\top}=X\left(X^{\top} X\right)^{-1}\left(X^{\top} X\right)\left(X^{\top} X\right)^{-1} X^{\top}=X\left(X^{\top} X\right)^{-1} X^{\top}=P
$$

因此 $P$ 是一个幕等矩阵, 所以 $P$ 是一个投影矩阵。
解释. 线性回归模型: $\hat{\boldsymbol{y}}=X^{\top}\left(X^{\top} X\right)^{-1} X^{\top} \boldsymbol{y}$ 。可以发现, $\hat{\boldsymbol{y}}$ 其实是 $\boldsymbol{y}$ 在线性空间的投影。



# HW4
#### 1. 
​	**[课本习题 4.1] 试证明对于不含冲突数据（即特征向量完全相同但标记不同）的训练集, 必存在与训练集一致 (即训练误差为 0 ) 的决策树。**

证明. (反证法) 假设不存在与训练集一致的决策树, 那么训练集训练得到的决策树必然含有冲突数据, 这与假设矛盾, 因此必然存在与训练集一致决策树。
#### 2.
​	**[课本习题 4.9] 试将 4.4.2 节对缺失值的处理机制推广到基尼指数的计算中去。解.**

$$
\begin{aligned}
\text{Gini}(D, a) & =\rho \times \text{Gini}\_\text{index}(\tilde{D}, a) \\
& =\rho \times \sum _{v=1}^V \tilde{r} _v \text{Gini}\left(\tilde{D}^v\right) \\
& =\rho \times \sum _{v=1}^V \tilde{r} _v\left(1-\sum _{i=1}^k \tilde{p} _k^2\right)
\end{aligned}
$$

#### 3.
​	**假设离散随机变量 $X \in\{1, \ldots, K\}$, 其取值为 $k$ 的概率 $P(X=k)=p_k$, 其摘为 $H(p)=-\sum_k p_k \log _2 p_k$ ，试用拉格朗日乘子法证明摘最大分布为均匀分布。**

证明.

$$
\begin{gathered}
L(p, \lambda)=-\sum _{i=1}^k p_i \log _2 p _i+\lambda\left(\sum _{i=1}^k p _i-1\right) \\
\frac{\partial L}{\partial p _i}=-\log _2 p _i-\frac{1}{\ln 2}+\lambda=0 \Longrightarrow p _1=p _2=\cdots=p _k=2^{\lambda-\frac{1}{\ln 2}} \\
\frac{\partial L}{\partial \lambda}=\sum _{i=1}^k p _i-1=0 \Longrightarrow p _1=p _2=\cdots=p _k=\frac{1}{k}
\end{gathered}
$$
#### 4.
​	**下表表示的二分类数据集，具有三个属性 A、B、C，样本标记为两类“+”, “-”。请运用学过的知识完成如下问题:**

|                  实例                   | $\mathrm{A}$  | $\mathrm{B}$  | $\mathrm{C}$ | 类别 |
| :-------------------------------------: | :-----------: | :-----------: | :----------: | :--: |
|                    1                    | $\mathrm{~T}$ | $\mathrm{~T}$ |     1.0      |  +   |
|                    2                    | $\mathrm{~T}$ | $\mathrm{~T}$ |     6.0      |  +   |
|                    3                    | $\mathrm{~T}$ | $\mathrm{~F}$ |     5.0      |  -   |
|                    4                    | $\mathrm{~F}$ | $\mathrm{~F}$ |     4.0      |  +   |
|                    5                    | $\mathrm{~F}$ | $\mathrm{~T}$ |     7.0      |  -   |
|                    6                    | $\mathrm{~F}$ | $\mathrm{~T}$ |     3.0      |  -   |
|                    7                    | $\mathrm{~F}$ | $\mathrm{~F}$ |     8.0      |  -   |
|                    8                    | $\mathrm{~T}$ | $\mathrm{~F}$ |     7.0      |  +   |
|                    9                    | $\mathrm{~F}$ | $\mathrm{~T}$ |     5.0      |  -   |
|                   10                    | $\mathrm{~F}$ | $\mathrm{~F}$ |     2.0      |  +   |

##### **4.1 整个训练样本关于类属性的摘是多少?**
解. 类别 + 的概率为 $p^{+}=\frac{5}{10}$, 类别 - 的概率为 $p^{-}=\frac{5}{10}$, 因此熵为

$$
\text{Ent}(D)=-p^{+} \log _2 p^{+}-p^{-} \log _2 p^{-}=1 \text {. }
$$
##### **4.2 数据集中 $\mathrm{A} 、 \mathrm{~B}$ 两个属性的信息增益各是多少?**

解.

$$
\begin{aligned}
\text{Gain}(D, A) & =\text{Ent}(D)-\sum _v \frac{\left|D^v\right|}{|D|} \text{Ent}\left(D^v\right) \\
& =1-\left(\frac{4}{10}\left(-\frac{3}{4} \log _2 \frac{3}{4}-\frac{1}{4} \log _2 \frac{1}{4}\right)+\frac{6}{10}\left(-\frac{4}{6} \log _2 \frac{4}{6}-\frac{2}{6} \log _2 \frac{2}{6}\right)\right) \\
& =0.125
\end{aligned}
$$

$$
\begin{aligned}
\text{Gain}(D, B) & =\text{Ent}(D)-\sum_v \frac{\left|D^v\right|}{|D|} \text{Ent}\left(D^v\right) \\
& =1-\left(\frac{5}{10}\left(-\frac{2}{5} \log _2 \frac{2}{5}-\frac{3}{5} \log _2 \frac{3}{5}\right)+\frac{5}{10}\left(-\frac{2}{5} \log _2 \frac{2}{5}-\frac{3}{5} \log _2 \frac{3}{5}\right)\right) \\
& =0.029
\end{aligned}
$$
##### **4.3 对于属性 C, 计算所有可能划分的信息增益?**

解. 可取划分点为 $\{1.5,2.5,3.5,4.5,5.5,6.5,7.5\}$, 然后对应划分信息增益为:

$$
\begin{gather}
\text{Gain}(D, C, 1.5)=1-\frac{9}{10}\left( -\frac{4}{9} \log _2 \frac{4}{9}-\frac{5}{9} \log _2 \frac{5}{9}\right) \approx 0.108 \\
\text{Gain}(D, C, 2.5)=1-\frac{8}{10}\left( -\frac{3}{8} \log _2 \frac{3}{8}-\frac{5}{8} \log _2 \frac{5}{8}\right) \approx 0.236 \\
\end{gather}
$$

$$
\begin{gather}
\text{Gain}(D, C, 3.5)=1-\left[ \frac{3}{10}\left( -\frac{2}{3} \log _2 \frac{2}{3}-\frac{1}{3} \log _2 \frac{1}{3}\right) +\frac{7}{10}\left( -\frac{3}{7} \log _2 \frac{3}{7}-\frac{4}{7} \log _2 \frac{4}{7}\right)\right] \approx 0.035 \\
\text{Gain}(D, C, 4.5)=1-\left[ \frac{4}{10}\left( -\frac{3}{4} \log _2 \frac{3}{4}-\frac{1}{4} \log _2 \frac{1}{4}\right) +\frac{6}{10}\left( -\frac{2}{6} \log _2 \frac{2}{6}-\frac{4}{6} \log _2 \frac{4}{6}\right)\right] \approx 0.125 \\
\text{Gain}(D, C, 5.5)=1-\left[ \frac{6}{10}\left( -\frac{3}{6} \log _2 \frac{3}{6}-\frac{3}{6} \log _2 \frac{3}{6}\right) +\frac{4}{10}\left( -\frac{2}{4} \log _2 \frac{2}{4}-\frac{2}{4} \log _2 \frac{2}{4}\right) \right] =0 \\
\text{Gain}(D, C, 6.5)=1-\left[ \frac{7}{10}\left( -\frac{4}{7} \log _2 \frac{4}{7}-\frac{3}{7} \log _2 \frac{3}{7}\right) +\frac{3}{10}\left( -\frac{1}{3} \log _2 \frac{1}{3}-\frac{2}{3} \log _2 \frac{2}{3}\right) \right] \approx 0.035 \\
\text{Gain}(D, C, 7.5)=1-\frac{9}{10}\left( -\frac{5}{9} \log _2 \frac{5}{9}-\frac{4}{9} \log _2 \frac{4}{9}\right) \approx 0.108
\end{gather}
$$
##### **4.4 根据 Gini 指数, $A$ 和 $B$ 两个属性哪个是最优划分?**

解.

$$
\begin{gather}
\text {Gini}\_\text{index}(D, A) & =\frac{4}{10}\left(1-\left(\frac{3}{4}\right)^2-\left(\frac{1}{4}\right)^2\right)+\frac{6}{10}\left(1-\left(\frac{2}{6}\right)^2-\left(\frac{4}{6}\right)^2\right)=0.417 \\
\text {Gini}\_\text{index}(D, B) & =\frac{5}{10}\left(1-\left(\frac{2}{5}\right)^2-\left(\frac{3}{5}\right)^2\right)+\frac{5}{10}\left(1-\left(\frac{3}{5}\right)^2-\left(\frac{2}{5}\right)^2\right) =0.48
\end{gather}
$$

因此， $A$ 是最优划分。
##### **4.5 采用算法 C4.5，构造决策树。**

解. - 指标：信息增益率, 不是信息增益
- 构造方法和构造结果不唯一, 建议大家构造前简述自己的构造方法, 可以拿一半的过程分。
初始: $D=\{1,2,3,4,5,6,7,8,9,10\}$
第一层划分:

$$
\text{Gain}\_\text{ratio}(D, A)=\frac{\text{Gain}(D, A)}{\text{IV}(D, A)}=\frac{0.125}{0.971}=0.129
$$

$$
\begin{gathered}
\text {Gain}\_\text{ratio}(D, B)=\frac{\text{Gain}(D, B)}{\text{IV}(D, B)}=\frac{0.029}{1}=0.029 \\
\text{Gain}\_ \text{ratio~}(D, C, 2.5)=\frac{\text{Gain}(D, C, 2.5)}{\text{IV}(D, C, 2.5)}=\frac{0.236}{0.722}=0.326
\end{gathered}
$$

选择 $(C, 2.5)$ 作为 $D$ 的划分, 得到 $D_{c \leq 2.5}^1=\{1,10\}, D_{c>2.5}^1=\{2,3,4,5,7,8,9\}$ 。第二层划分: 因为 $D_{c \leq 2.5}^1$ 元素类别一致, 不再划分, 主要针对 $D_{c>2.5}^1$ 继续划分。

$$
\begin{gather}
\text {Gain} _\text{ratio}\left(D _{c\gt 2.5}^1, A\right)=\frac{\text{Gain}\left(D _{c\gt 2.5}^1, A\right)}{\text{IV}\left(D _{c\gt 2.5}^1, A\right)}=\frac{0.159}{0.954}=0.167 \\
\text {Gain} _\text{ratio}\left(D _{c\gt 2.5}^1, B\right)=\frac{\text{Gain}\left(D _{c\gt 2.5}^1, B\right)}{\text{IV}\left(D _{c\gt 2.5}^1, B\right)}=\frac{0.049}{1}=0.049 \\
\text {Gain} _\text{ratio}\left(D _{c\gt 2.5}^1, C, 3.5\right)=\frac{\text{Gain}\left(D _{c\gt 2.5}^1, C, 3.5\right)}{\text{IV}\left(D _{c\gt 2.5}^1, C, 3.5\right)}=\frac{0.092}{0.544}=0.169
\end{gather}
$$

选择 $(C, 3.5)$ 作为 $D_{c\gt 2.5}^1$ 的划分, 得到 $D_{2.5 \lt c \leq 3.5}^2=\{6\}, D_{c \geq 3.5}^2=\{2,3,4,5,7,8,9\}$ 。

第三层划分: 以此类推...

![image-20231022145317109](HW3&4%E8%A7%A3%E7%AD%94.assets/image-20231022145317109.png)
