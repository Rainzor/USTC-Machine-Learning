
## 第9次作业

1. 记 $\operatorname{err}^*(\boldsymbol{x})=1-\max _{c \in \mathcal{Y}} P(c \mid \boldsymbol{x}), \operatorname{err}(\boldsymbol{x})=1-\sum_c P(c \mid \boldsymbol{x}) P(c \mid \boldsymbol{z})$, 其中 $\boldsymbol{z}$ 为 $\boldsymbol{x}$ 的最近邻, 试证明在样本无穷多时
$$
\operatorname{err}^*(\boldsymbol{x}) \leq \operatorname{err}(\boldsymbol{x}) \leq \operatorname{err}^*(\boldsymbol{x})\left(2-\frac{|\mathcal{Y}|}{|\mathcal{Y}|-1} \times \operatorname{err}^*(\boldsymbol{x})\right)
$$

提示：柯西-施瓦兹不等式 $\left(\sum_i a_i\right)^2 \leq n\left(\sum_i a_i^2\right)$ 。
证明. 先证明左边不等式:
$$
\begin{aligned}
\operatorname{err}^*(\boldsymbol{x}) & =1-\max _{c \in \mathcal{Y}} P(c \mid \boldsymbol{x})=1-\max _{c \in \mathcal{Y}} P(c \mid \boldsymbol{x}) \cdot \sum_c P(c \mid \boldsymbol{z}) \\
& =1-\sum_c \max _{c \in \mathcal{Y}} P(c \mid \boldsymbol{x}) \cdot P(c \mid \boldsymbol{z}) \\
& \leq 1-\sum_c P(c \mid \boldsymbol{x}) \cdot P(c \mid \boldsymbol{z})=\operatorname{err}^*(\boldsymbol{x})
\end{aligned}
$$

令 $c^*=\arg \max _c P(c \mid \boldsymbol{x})$, 再证明右边不等式:
$$
\begin{aligned}
\operatorname{err}^*(\boldsymbol{x}) & =1-\sum_c P(c \mid \boldsymbol{x}) \cdot P(c \mid \boldsymbol{z}) \simeq 1-\sum_c P(c \mid \boldsymbol{x})^2 \\
& \leq 1-P\left(c^* \mid \boldsymbol{x}\right)^2-\sum_{c \neq c^*} P(c \mid \boldsymbol{x})^2 \\
& \leq 1-P\left(c^* \mid \boldsymbol{x}\right)^2-\frac{1}{|\mathcal{Y}|-1}\left(\sum_{c \neq c^*} P(c \mid \boldsymbol{x})\right)^2 \\
& =1-P\left(c^* \mid \boldsymbol{x}\right)^2-\frac{1}{|\mathcal{Y}|-1}\left(1-P\left(c^* \mid \boldsymbol{x}\right)\right)^2 \\
& =\left(1-P\left(c^* \mid \boldsymbol{x}\right)\right) \cdot\left(1+P\left(c^* \mid \boldsymbol{x}\right)-\frac{1}{|\mathcal{Y}|-1}\left(1-P\left(c^* \mid \boldsymbol{x}\right)\right)\right) \\
& =\left(1-P\left(c^* \mid \boldsymbol{x}\right)\right) \cdot\left(2-\frac{|\mathcal{Y}|}{|\mathcal{Y}|-1}\left(1-P\left(c^* \mid \boldsymbol{x}\right)\right)\right) \\
& =\operatorname{err}^*(\boldsymbol{x}) \cdot\left(2-\frac{|\mathcal{Y}|}{|\mathcal{Y}|-1} \cdot \operatorname{err}^*(\boldsymbol{x})\right)
\end{aligned}
$$

综上所述, 证毕!



2. 在实践中, 协方差矩阵 $\mathrm{XX}^{\top}$ 的特征值分解常由中心化后的样本矩阵 $\mathrm{X}$ 的奇异值分解替代, 试述其原因。

   解. 仅供参考, 言之成理即可。
   令 $\mathbf{X X}^{\top}$ 的特征值分解为
   $$
   \mathbf{X} \mathbf{X}^{\top}=\mathbf{Y} \boldsymbol{\Lambda} \mathbf{Y}^{\top}
   $$

   令 $\mathbf{X}$ 的奇异值分解为 $\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\top}$, 可得
   $$
   \mathbf{X} \mathbf{X}^{\top}=\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\top}\left(\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\top}\right)^{\top}=\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\top} \mathbf{V} \boldsymbol{\Sigma}^{\top} \mathbf{U}^{\top}
   $$

   因为 $\mathbf{X}$ 是经过中心化的样本矩阵, 因此 $\mathbf{V}^{\top} \mathbf{V}=\mathbf{I}, \mathbf{U}^{\top} \mathbf{U}=\mathbf{I}$, 所以
   $$
   \mathbf{X} \mathbf{X}^{\top}=\mathbf{U}\left(\boldsymbol{\Sigma} \boldsymbol{\Sigma}^{\top}\right) \mathbf{U}^{\top}
   $$

   如果令 $\mathbf{Y}=\mathbf{U} 、 \boldsymbol{\Lambda}=\boldsymbol{\Sigma} \boldsymbol{\Sigma}^{\top}$, 不难发现式(1)和(2)是等价的, 也是就是协方差矩阵的特征值分解与中心化后的样本矩阵的奇异值分解其实是等价的。
   除此外, 相较于特征值分解, 奇异值分解的运算要更加高效, 节省存储空间。


3. $$
   \begin{array}{cl}
   \max _{\mathbf{W}} & \operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right) \\
   \text { s.t. } & \mathbf{W}^{\top} \mathbf{W}=\mathbf{I}_{d^{\prime}}
   \end{array}
   $$

   解. 先将问题转化为等价问题,
   $$
   \begin{array}{cl}
   \min _{\mathbf{W}} & -\operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right) \\
   \text { s.t. } & \mathbf{W}^{\top} \mathbf{W}=\mathbf{I}_{d^{\prime}}
   \end{array}
   $$

   然后使用拉格朗日乘子法, 构造拉格朗日函数
   $$
   L(\mathbf{W}, \boldsymbol{\Lambda})=-\operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right)+\operatorname{tr}\left(\boldsymbol{\Lambda}\left(\mathbf{W}^{\top} \mathbf{W}-\mathbf{I}_{\mathbf{d}^{\prime}}\right)\right)
   $$

   其中, $\boldsymbol{\Lambda}=\operatorname{diag}\left(\lambda_1, \cdots, \lambda_{d^{\prime}}\right)$, 于是令
   $$
   \begin{aligned}
   \frac{\partial L}{\partial \mathbf{W}} & =-\frac{\partial}{\partial \mathbf{W}} \operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right)+\frac{\partial}{\partial \mathbf{W}} \operatorname{tr}\left(\boldsymbol{\Lambda}\left(\mathbf{W}^{\top} \mathbf{W}-\mathbf{I}_{\mathbf{d}^{\prime}}\right)\right) \\
   & =-2 \mathbf{X} \mathbf{X}^{\top} \mathbf{W}+2 \mathbf{W} \mathbf{\Lambda} \\
   & =0
   \end{aligned}
   $$

   解得
   $$
   \mathbf{X} \mathbf{X}^{\top} \mathbf{W}=\mathbf{W} \mathbf{\Lambda}
   $$

   这意味着
   $$
   \mathbf{X X}^{\top} \boldsymbol{w}_i=\lambda_i \boldsymbol{w}_i
   $$

   也就是说, 取 $\mathbf{X} \mathbf{X}^{\top}$ 的最大的前 $d^{\prime}$ 个特征值所对应的特征向量即可得 $\mathbf{W}$ 。将之代入到目标函数即可得
   $$
   \operatorname{tr}\left(\mathbf{W}^{\top} \mathbf{X} \mathbf{X}^{\top} \mathbf{W}\right)=\sum_{i=1}^{d^{\prime}} \lambda_{d^{\prime}}
   $$

## 第10次作业
1. [课本习题 11.5] 结合图 11.2, 试举例说明 $\mathbf{L}_1$ 正则化在何种情形下不能产生稀疏解。解. 如图所示, 当平方差误差项等值线的斜率较大的时候, 其与 $L_1$ 范数等值线的交点就不再位于坐标轴上, 因此将无法产生稀疏解。
   ![image-20240104164914475](HW9&10%E8%A7%A3%E7%AD%94.assets/image-20240104164914475.png)

   ![image-20240104165050551](HW9&10%E8%A7%A3%E7%AD%94.assets/image-20240104165050551.png)

2. [课本习题 11.7] 试述直接求解 $\mathbf{L}_0$ 范数正则化会遇到的困难。

   解. $\mathrm{L}_0$ 范数是统计向量非零元素的个数, 不连续、不可微、非凸, 无法通过凸优化的方式求解,需要采用遍历方式才能找到最优解, 因此难度是 NP-难的。

3. [PPT 20 页] 证明回归和对率回归的损失函数的梯度是否满足 L-Lipschitz 条件, 并求出 $\mathbf{L}$ 。

   证明. 先证明线性回归函数, 其损失函数为
   $$
   E(\boldsymbol{w})=(\boldsymbol{y}-\mathbf{X} \boldsymbol{w})^{\top}(\boldsymbol{y}-\mathbf{X} \boldsymbol{w})
   $$

   不难发现该函数为凸函数, 其微分算子 $\nabla E$ 表示为
   $$
   \nabla E(\boldsymbol{w})=2 \mathbf{X}^{\top}(\mathbf{X} \boldsymbol{w}-\boldsymbol{y})
   $$

   对于 $\forall \boldsymbol{w}, \boldsymbol{w}^{\prime}$, 都有
   $$
   \begin{aligned}
   \left\|\nabla E(\boldsymbol{w})-\nabla E\left(\boldsymbol{w}^{\prime}\right)\right\|_2 & =\left\|2 \mathbf{X}^{\top} \mathbf{X}\left(\boldsymbol{w}-\boldsymbol{w}^{\prime}\right)\right\|_2 \\
   & \leq 2\left\|\mathbf{X}^{\top} \mathbf{X}\right\|_2 \cdot\left\|\boldsymbol{w}-\boldsymbol{w}^{\prime}\right\|_2
   \end{aligned}
   $$

   令 $L=2\left\|\mathbf{X}^{\top} \mathbf{X}\right\|_2>0$, 可以发现线性回归函数的损失函数满足 L-Lipschitz 条件。
   接着证明对率回归函数, 其损失函数为
   $$
   \ell(\boldsymbol{\beta})=\sum_{i=1}^m\left(-y_i \boldsymbol{\beta}^{\top} \boldsymbol{x}_i+\ln \left(1+e^{\boldsymbol{\beta}^{\top} \boldsymbol{x}_i}\right)\right)
   $$

   该函数是关于 $\boldsymbol{\beta}$ 的高阶可导连续凸函数, 其微分算子 $\nabla \ell$ 表示为
   $$
   \nabla \ell(\boldsymbol{\beta})=\sum_{i=1}^m\left(-y_i \boldsymbol{x}_i+\frac{\boldsymbol{x}_i e^{\boldsymbol{\beta}^{\top} \boldsymbol{x}_i}}{1+e^{\boldsymbol{\beta}^{\top} \boldsymbol{x}_i}}\right)=\sum_{i=1}^m\left(\frac{1}{1+e^{-\boldsymbol{\beta}^{\top} \boldsymbol{x}_i}}-y_i\right) \boldsymbol{x}_i
   $$

   对于 $\forall \boldsymbol{\beta}, \boldsymbol{\beta}^{\prime}$ ，都有
   $$
   \left\|\nabla \ell(\boldsymbol{\beta})-\nabla \ell\left(\boldsymbol{\beta}^{\prime}\right)\right\|_2=\left\|\sum_i\left(\frac{1}{1+e^{-\boldsymbol{\beta}^{\top} \boldsymbol{x}_i}}-\frac{1}{1+e^{-\boldsymbol{\beta}^{\prime \top} \boldsymbol{x}_i}}\right) \boldsymbol{x}_i\right\|_2
   $$

   注意到 Sigmod 函数 $f(x)=\frac{1}{1+e^{-x}}$ 上任意两点连线的斜率小于等于 $f^{\prime}(0)$, 因此可知
   $$
   \frac{1}{1+e^{-\boldsymbol{\beta}^{\top} \boldsymbol{x}_i}}-\frac{1}{1+e^{-\boldsymbol{\beta}^{\prime \top} \boldsymbol{x}_i}} \leq\left.\left(\frac{1}{1+e^{-\boldsymbol{\beta}^{\top} \boldsymbol{x}_i}}\right)^{\prime}\right|_{\boldsymbol{\beta}^{\top} \boldsymbol{x}_i=0}\left(\boldsymbol{\beta}^{\top}-\boldsymbol{\beta}^{\prime^{\top}}\right) \boldsymbol{x}_i=\frac{1}{4} \boldsymbol{x}_i^{\top}\left(\boldsymbol{\beta}-\boldsymbol{\beta}^{\prime}\right)
   $$

   因此可得
   $$
   \begin{aligned}
   \left\|\nabla \ell(\boldsymbol{\beta})-\nabla \ell\left(\boldsymbol{\beta}^{\prime}\right)\right\|_2 & \leq\left\|\sum_i \frac{1}{4} \boldsymbol{x}_i^{\top}\left(\boldsymbol{\beta}-\boldsymbol{\beta}^{\prime}\right) \boldsymbol{x}_i\right\|_2 \\
   & \leq \frac{1}{4}\left\|\sum_i \boldsymbol{x}_i^{\top} \boldsymbol{x}_{\boldsymbol{i}}\right\|_2 \cdot\left\|\boldsymbol{\beta}-\boldsymbol{\beta}^{\prime}\right\|_2
   \end{aligned}
   $$

   令 $L=\frac{1}{4}\left\|\sum_i \boldsymbol{x}_i^{\top} \boldsymbol{x}_{\boldsymbol{i}}\right\|_2>0$, 可以发现对率回归函数的损失函数满足 L-Lipschitz 条件。